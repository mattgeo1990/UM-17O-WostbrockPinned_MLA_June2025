formatCount = 0
for (reactor.file.number in 30:30) { # This is the way to only reduce one reactor at a time.
#for (reactor.file.number in 1:length(reactor.file.names)) { # can be used for either UM or JHU files (this will reduce all reactors)
# for(reactor.file.number in 1:length(reactor.file.names)) { # for UM files
# for(reactor.file.number in 1:length(reactor.file.names.JHU)) { # for JHU files
# On the first loop, format all of the data (this is quick, can be done every time)
if (formatCount == 0){
# Define path to the raw data files and data formatting function
path.to.format.func <-
paste(path.data.red,"/","0000_LabFileFormatting","/",sep="")
source(paste(path.to.format.func, "ReactorSpreadsheetFormatter_func_v2KA_acf.r", sep = ""))
# Run formatting function (no graphics, just formats lab excel files into a common CSV format)
ReactorSpreadsheetFormatter_func(reactor.file.names0)
# And flag this if statement to never run again
formatCount = 1
}
print(paste("reducing Reactor ", reactor.file.number,sep=""))
# Clear before each reactor reduction - to keep different reactors from interacting
rm(list = ls()[!ls() %in% c("reactor.file.names", "reactor.file.number", "formatCount",
"path.data.red","tert.cor.option","reactor.file.names.JHU","JHUorUM")])
######### data file locations setup ###########################################
# importing path
path.in <- paste(path.data.red,"/","0000_LabFileFormatting/001_Reactor Spreadsheet Formatted","/",sep="")
# Output path
path.out <- paste(path.data.red,"/","0002_LabFileCorrectedOutput","/",sep="")
# define path to the accepted standard values spreadsheet
path.to.accept.std <-
paste(path.data.red,"/","0001_ReductionCode","/",sep="")
accepted.std.file = "acceptedStds.csv"
# define path to the accepted d18O-mineral values spreadsheet (traditional d18O(CO2/CaCO3) measurements used for secondary normalization, see Passey 2014)
path.to.trad.master <-
paste(path.data.red,"/","0001_ReductionCode","/",sep="")
# Name of the spreadsheet
trad.master.file = "d18O_secondarycorrection.xlsx"
# Define path to the functions needed for this program
path.to.functions <-
paste(path.data.red,"/","0001_ReductionCode/ReductionCodeFunctions","/",sep="")
# Function sources
source(paste(path.to.functions, "correct.SMOW.SLAP.basic.R", sep = ""))
source(paste(path.to.functions, "correct.SMOW.SLAP.linearSMOW.R", sep = ""))
source(paste(path.to.functions, "segment.finder.R", sep = ""))
# Parameters to automatically decide which correction scheme to use for the data. The program starts by assuming the basic SMOW-SLAP correction, then does a linear correction above a certain r-sqared value in SMOW data, then does a segmented correction (with basic and linear corrections decided within each section) if assigned in the lab spreadsheet (flag.major).
threshold.rsq = 0.00 # R-squared threshold to use a linear correction for data reduction instead of basic correction. Zero value always prefers linear correction.
threshold.singlePrime <-
5 # The per mil difference between samples beyond which at least a single prime is required (or else data is flagged out)
threshold.doublePrime <-
10 # The per mil difference between samples beyond which at least two primes are required (or else data is flagged out)
######### Start reduction ###########################################
############################################################################################
# Import files
############################################################################################
#import data
setwd(path.in)
# Which data file to import?
if (JHUorUM == 1){ # setup to read in a JHU file
data.file <-
paste("JHUsession_", reactor.file.names.JHU[reactor.file.number], "Final.csv", sep =
"")
}else{ # setup to read in a UM file
data.file <-
paste("Reactor", reactor.file.names[reactor.file.number], "Final.csv", sep =
"")
}
compiled.nu.data_0 <-
read.csv(data.file, stringsAsFactors = FALSE, header = TRUE)
# Import the accepted standard values
std.accepted <-
read.csv(
paste(path.to.accept.std, accepted.std.file, sep = ""),
stringsAsFactors = FALSE,
header = TRUE
)
# Import the master list of d18O-mineral values
trad.master <-
read_excel(paste(path.to.trad.master, trad.master.file, sep = ""),
skip = 0)
############################################################################################
# Initial data formatting (flagged analyses, segmented reactors)
############################################################################################
# Change IPL.num from character to numeric
compiled.nu.data_0$IPL.num <- as.numeric(compiled.nu.data_0$IPL.num)
# Create the overall flag column, which combines flags from flag.analysis and from samples that were not primed correctly. These flagged analyses cannot be trusted and are removed from the dataset.
compiled.nu.data <- subset(compiled.nu.data_0, group.num > 0 & !is.na(d.18O.prelim))
compiled.nu.data$test.d.18O.prelim <-
compiled.nu.data$flag.analysis * 0 - 9999 # some fill values for the test.d.18O.prelim column
compiled.nu.data$flag.analysis[is.na(compiled.nu.data$flag.analysis)] <-
0 # NA values mean no flag
compiled.nu.data$flag.d18O <-
compiled.nu.data$flag.analysis * 0 + 1 # default to reject (ones) for start
compiled.nu.data$primes[is.na(compiled.nu.data$primes)] <-
0 # NA values mean no prime
for (dk in 1:nrow(compiled.nu.data)) {
# calculate the test value
if (dk == 1) {
# first row, nothing to test against
compiled.nu.data$test.d.18O.prelim[dk] = 6 # this value forces code to always require at least one prime for the first analysis
} else {
compiled.nu.data$test.d.18O.prelim[dk] = abs(compiled.nu.data$d.18O.prelim[dk] -
compiled.nu.data$d.18O.prelim[dk - 1])
}
if (compiled.nu.data$test.d.18O.prelim[dk] < threshold.singlePrime) {
# no check required, sample jump is small
compiled.nu.data$flag.d18O[dk] <- 0 # accept the row
} else if (compiled.nu.data$test.d.18O.prelim[dk] > threshold.singlePrime &
compiled.nu.data$test.d.18O.prelim[dk] < threshold.doublePrime) {
#  check required, sample jump large enough to require a single prime
if (compiled.nu.data$primes[dk] > 0) {
# check passed
compiled.nu.data$flag.d18O[dk] <- 0 # accept the row
}
} else if (compiled.nu.data$test.d.18O.prelim[dk] > threshold.doublePrime) {
#  check required, sample jump large enough to require a double prime
if (compiled.nu.data$primes[dk] > 1) {
# check passed
compiled.nu.data$flag.d18O[dk] <- 0 # accept the row
} else {
compiled.nu.data$flag.d18O[dk] <- 2
}
}
}
# Combine flag.analytical and flag.d18O
compiled.nu.data$flag <-
compiled.nu.data$flag.d18O + compiled.nu.data$flag.analysis
test.flag <- compiled.nu.data$flag > 0 # was it flagged?
compiled.nu.data$flag[test.flag] <- 1 # anything flagged gets a 1
# Make D17O.data, a subset of the whole spreadsheet. Bad analyses and lines without a group number (run comments) are removed.
D17O.data <- subset(compiled.nu.data, flag == 0 & group.num > 0)
# If flag.major ~=0, create segment.IPL (controls segmented runs) from the flag.major column. Segments are only created from a user-defined column (flag.major) in the lab spreadsheet. These must be major events - e.g., changing the filament.
segment.IPL <-
matrix(-9999, nrow = 1, ncol = 2) # Default value for no segments
compiled.nu.data_0$flag.major[is.na(compiled.nu.data_0$flag.major)] <-
0 # NA values mean no flag
if (sum(compiled.nu.data_0$flag.major) > 0) {
# if there are any segments
# Find the IPL number after each break
flag.major.test2 <-
which(compiled.nu.data_0$flag.major %in% 1) # find the breaks
segment.IPL.numList <-
as.numeric(compiled.nu.data_0$IPL.num[flag.major.test2 + 1]) # start of each new segment
# add on beginning and end of the reactor
subset.compiled.nu.data_0 <-
subset(compiled.nu.data_0, group.num > 0)
segment.IPL.numList <-
as.numeric(c(
min(subset.compiled.nu.data_0$IPL.num),
segment.IPL.numList,
max(subset.compiled.nu.data_0$IPL.num)
))
# For each break, set up the min/max IPL.nums
segment.IPL = matrix(NA,
nrow = length(segment.IPL.numList) - 1,
ncol = 2)
# Now fill the matrix
for (mm in 1:(length(segment.IPL.numList) - 1)) {
cur.starter <- segment.IPL.numList[mm]
if (mm < length(segment.IPL.numList) - 1) {
# this is not the end
cur.ender <- segment.IPL.numList[mm + 1] - 1
} else {
# This is the last segment - end at the last IPL. number
cur.ender <- segment.IPL.numList[mm + 1]
}
segment.IPL[mm, 1:2] <- c(cur.starter, cur.ender)
}
# Revise the segment.IPL matrix. Numbers are either matched up to their corresponding IPL number in the flag-cleared data list or are rolled to the next highest number available.
for (mm in 1:nrow(segment.IPL)) {
for (mmm in 1:ncol(segment.IPL)) {
safe.count <- -999 # just a default starting value
match.IPL = which(as.numeric(D17O.data$IPL.num) == segment.IPL[mm, mmm])
if (length(match.IPL) < 1) {
# there is not a match right away
safe.count <- 0
# Find next nearest IPL.num above this one and replace in segment.IPL
while (length(match.IPL) < 1) {
# there is not a match
safe.count <- safe.count + 1
if (mmm == 1) {
# this is a start segment, count forwards
segment.IPL[mm, mmm] <- segment.IPL[mm, mmm] + 1
} else if (mmm == 2) {
# this is an end segment, count backwards
segment.IPL[mm, mmm] <- segment.IPL[mm, mmm] - 1
}
match.IPL <-
which(as.numeric(D17O.data$IPL.num) == segment.IPL[mm, mmm])
if (safe.count > 1000) {
# If the segment is not real, break this look
print("error in making segment.IPL")
break
}
}
} else {
# there is a match right away
safe.count <-
0 # happens often for the end of the run (e.g., last SMOW/SLAP)
}
print(
paste(
"segment correction: IPL.num match for ",
segment.IPL[mm, mmm],
" found in ",
safe.count,
" loop(s)",
sep = ""
)
) # This is a printout that checks how far the program looked for a match. Used for troubleshooting.
}
}
}
# # Add a column that contains the reactor segment number - useful for making SI Appendices
D17O.data$reactor.segment <- 1 # There is always at least one segment
for (k in 1:nrow(segment.IPL)){
# Test for the data that match the segment and add them
curRows <- as.numeric(D17O.data$IPL.num)>=segment.IPL[k,1] & as.numeric(D17O.data$IPL.num)<=segment.IPL[k,2]
D17O.data$reactor.segment[curRows] <- k
}
############################################################################################
# Plotting variables and output information
############################################################################################
# Define the reactor prefix prefix (reactor.ID)
prefix <-
unique(D17O.data$reactor.ID)[!(unique(D17O.data$reactor.ID) == "")]
#setup colors for plots
#possible standards
standards <-
c(
"SMOW",
"SLAP",
"GISP",
"USGS45",
"USGS46",
"USGS47",
"USGS48",
"USGS49",
"USGS50",
"102-GC-AZ01",
"IAEA-C1",
"GON06-OES",
"NBS-18",
"NBS-19",
"IAEA-603"
# ,"USGS80",
# "B2207",
# "USGS81",
# "RSP-1"
)
#non-standard water samples
unknown.waters <-
unique(subset(D17O.data, Type.1 == "Water")$Type.2)
unknown.carbonates <-
unique(subset(D17O.data, Type.1 == "Carbonate")$Type.2)
plot.col.byCorr <-
c(
"black",
"darkcyan",
"lightblue",
"darkgreen",
"blueviolet",
"goldenrod",
"darkorange3",
"firebrick",
"orchid",
rep("grey40", length(unknown.waters)),
rep("grey80", length(unknown.carbonates))
)
# Following section is unused, names are made below as needed.
# plot.col.byCorr.names = c("basic",
#                           "linear",
#                           "seg.1",
#                           "seg.2",
#                           "seg.3",
#                           "seg.4",
#                           "seg.5",
#                           "seg.6",
#                           "seg.7")
# names(plot.col.byCorr) = plot.col.byCorr.names
plotting.colors.byStd <-
c(
"black",
"darkcyan",
"lightblue",
"darkgreen",
"blueviolet",
"goldenrod",
"darkorange3",
"firebrick",
"orchid",
"yellow",
"blue",
"green",
"red",
"purple",
"blue",
"aquamarine",
"aquamarine4",
"brown",
"brown1",
rep("grey40", length(unknown.waters)),
rep("grey80", length(unknown.carbonates))
)
names(plotting.colors.byStd) = c(
"SMOW",
"SLAP",
"GISP",
"USGS45",
"USGS46",
"USGS47",
"USGS48",
"USGS49",
"USGS50",
"102-GC-AZ01",
"IAEA-C1",
"GON06-OES",
"NBS-18",
"NBS-19",
"IAEA-603",
"USGS80",
"B2207",
"USGS81",
"RSP-1",
unknown.waters,
unknown.carbonates
)
# plotting.shapes.byStd <-
#   c(15, 16, 17, 18, 19, 20,
#     15, 16, 17, 18, 19, 20,
#     15, 16, 17, 18, 19, 20,
#     15, 16, 17, 18, 19, 20,
#     15)
plotting.shapes.byStd <- matrix(0,nrow=length(plotting.colors.byStd),1)
m <- 14
for (r in 1:length(plotting.colors.byStd)){
m <- m+1
if (m==21){
m<-15
}
plotting.shapes.byStd[r] <- m
}
names(plotting.shapes.byStd) = c(
"SMOW",
"SLAP",
"GISP",
"USGS45",
"USGS46",
"USGS47",
"USGS48",
"USGS49",
"USGS50",
"102-GC-AZ01",
"IAEA-C1",
"GON06-OES",
"NBS-18",
"NBS-19",
"IAEA-603",
"USGS80",
"B2207",
"USGS81",
"RSP-1",
unknown.waters,
unknown.carbonates
)
# Set overall theme
themeA <- theme(
plot.background = element_rect(fill = "white"),
panel.background = element_rect(fill = "white", colour="black",size=2),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text=element_text(size=20),
axis.title=element_text(size=20),
axis.ticks.length = unit(0.5,"lines"),
axis.ticks = element_line(size=1),
plot.margin = margin(20,20,20,20),
legend.position = "none"
)
############################################################################################
# Begin data reduction (basic, linear, segmented, final). Final=preferred by code in order of basic -> linear -> segmented. Note that threshold values are set at top.
############################################################################################
# ```
# Input data file = `r data.file`
# Reactor ID = `r prefix`
## BASIC - Do basic SMOW-SLAP reduction.
D17O.data.basic.cor = correct.SMOW.SLAP.basic(D17O.data)
D17O.final = D17O.data.basic.cor # BASIC correction is always assumed first
D17O.final.line.stats = NA
basic.linear = -9999 # This is a default flag that says not to do the segment correction
cat = 1
## LINEAR DRIFT - reduction for case where there is a linear drift in the standards. Regression is done on the SMOW analyses.
D17O.data.linearSMOW.cor.HOLD = correct.SMOW.SLAP.linearSMOW(D17O.data)
names(D17O.data.linearSMOW.cor.HOLD) <-
c("D17O.data.linearSMOW.cor",
"D17O.data.line.stats.linearSMOW")
list2env(D17O.data.linearSMOW.cor.HOLD, envir = .GlobalEnv)
if (D17O.data.line.stats.linearSMOW[3, 2] > threshold.rsq &
D17O.data.line.stats.linearSMOW[3, 3] > threshold.rsq) { # for d33 and d34, respectively
D17O.final = D17O.data.linearSMOW.cor # if threshold met, switch to the linear correction
D17O.final.line.stats = D17O.data.line.stats.linearSMOW
}
## SEGMENTS - reduction for case where there is a large jump in the SMOW values mid-reactor. Before and after the jump(s) are reduced separately and results are concatenated (i.e., you will get overlapping SMOW, and potentially SLAP, sets).
# Now run the segment correction if there were identified jumps
if (segment.IPL[1, 1] != -9999) {
basic.linear = 1 # This overrides the default setting, identifies segment correction exists
# Run the BASIC and LINEAR reductions for each segment.
for (k in 1:nrow(segment.IPL)) {
IPL.num.list.b = as.numeric(D17O.data$IPL.num) # note that this produces a warning message as R will convert all characters (e.g., "the reactor did XYZ") into N/A values. Doesn't seem to affect the indexing though.
# Note that this formulation deals with some of the weirly implemented IPL.num
starter <- which(IPL.num.list.b == segment.IPL[k, 1])
ender <- which(IPL.num.list.b == segment.IPL[k, 2])
D17O.data.segment = D17O.data[starter:ender, ]
D17O.data.segment <-
D17O.data.segment[!is.na(D17O.data.segment$IPL.num),] # delete NA rows - they are not needed.
# correct segments using BASIC correction
segment.cor.HOLD.basic <-
correct.SMOW.SLAP.basic(D17O.data.segment)
if (k == 1) {
# make the lists on the first run through
D17O.data.segment.cor.basic <-
vector(mode = "list", length = nrow(segment.IPL))
}
# Now fill the list with the corrected data
D17O.data.segment.cor.basic[[k]] <- segment.cor.HOLD.basic
# Also correct segments using LINEAR correction
segment.cor.HOLD.linear <-
correct.SMOW.SLAP.linearSMOW(D17O.data.segment)
if (k == 1) {
# make the lists on the first run through (these contain all corrections done for the data set)
D17O.data.segment.cor.linear <-
vector(mode = "list", length = nrow(segment.IPL))
D17O.data.segment.cor.line.stats.linear <-
vector(mode = "list", length = nrow(segment.IPL))
}
# Now fill the list with the corrected data
D17O.data.segment.cor.linear[[k]] <-
segment.cor.HOLD.linear[["name1"]]
D17O.data.segment.cor.line.stats.linear[[k]] <-
segment.cor.HOLD.linear[["name2"]]
# Now automatically decide whether to use BASIC or LINEAR correction for each segment. With threshold.rsq=0, this defaults to using the LINEAR correction
temp.rsq33 <-
D17O.data.segment.cor.line.stats.linear[[k]][3, 2]
temp.rsq34 <-
D17O.data.segment.cor.line.stats.linear[[k]][3, 3]
if (k == 1) {
# make the lists on the first run through
D17O.data.segment.cor.all <-
vector(mode = "list", length = nrow(segment.IPL))
D17O.data.segment.cor.line.stats.all <-
vector(mode = "list", length = nrow(segment.IPL))
}
if (temp.rsq33 > threshold.rsq &
temp.rsq34 > threshold.rsq) {
# did meet LINEAR threshold
D17O.data.segment.cor.all[[k]] <-
segment.cor.HOLD.linear[["name1"]]
D17O.data.segment.cor.line.stats.all[[k]] <-
segment.cor.HOLD.linear[["name2"]]
} else {
# did not meet LINEAR threshold
D17O.data.segment.cor.all[[k]] <- segment.cor.HOLD.basic
D17O.data.segment.cor.line.stats.all[[k]] <- NA
}
}
D17O.final = D17O.data.segment.cor.all
D17O.final.line.stats = D17O.data.segment.cor.line.stats.all
}
############################################################################################
# Put all D17O-corrected data in a single list (primary normalization complete)
############################################################################################
# Set up a list to contain all of the data for easier plotting
# set up empty list with correct number of spots
if (basic.linear == -9999) {
# no segments identified
data.cor = vector(mode = "list", length = 2)
} else if (basic.linear >= 1) {
# Segments were identified
data.cor = vector(mode = "list", length = 2 + nrow(segment.IPL) * 2) # *2 to keep both the basic and linear segmented corrections
}
data.cor[[1]] = D17O.data.basic.cor
data.cor[[2]] = D17O.data.linearSMOW.cor
if (basic.linear == 1) {
# segmented correction exists
# All of the segment-basic corrected data
for (k in 3:(2 + nrow(segment.IPL))) {
data.cor[[k]] = D17O.data.segment.cor.basic[[k - 2]]
}
# All of the segment-linear corrected data
for (k in (2 + nrow(segment.IPL) + 1):length(data.cor)) {
data.cor[[k]] = D17O.data.segment.cor.linear[[k - (2 + nrow(segment.IPL))]]
}
}
# Set up a list to contain all of the line stats for reference
data.cor.line.stats <-
vector(mode = "list", length = length(data.cor))
data.cor.line.stats[[1]] = NA # There are no line stats for the basic correction
data.cor.line.stats[[2]] = D17O.data.line.stats.linearSMOW
if (basic.linear == 1) {
# segmented correction exists
# All of the segment-basic corrected data
for (k in 3:(2 + nrow(segment.IPL))) {
data.cor.line.stats[[k]] = NA # There are no line stats for the basic correction
}
# All of the segment-linear corrected data
for (k in (2 + nrow(segment.IPL) + 1):length(data.cor)) {
data.cor.line.stats[[k]] = D17O.data.segment.cor.line.stats.linear[[k -
(2 + nrow(segment.IPL))]]
}
}
# Data reduction folder data path. This address should lead to "Data Reduction Procedure." No final "/" is needed.
# If the auto-finding function doesn't work, here is an example way to statically specify the address
# path.data.red <- "D:/Documents/000_Michigan/Laboratory Data Files/Data Reduction Procedure"
setwd(dirname(rstudioapi::getActiveDocumentContext()$"/Users/annefetrow/Library/CloudStorage/GoogleDrive-fetrowa@gmail.com/My Drive/UMich/IPL/Reactor Reductions/UM 17O WostbrockPinned"))
path.data.red <- dirname(getwd())
path.data.red <- "/Users/annefetrow/Library/CloudStorage/GoogleDrive-fetrowa@gmail.com/My Drive/UMich/IPL/Reactor Reductions/UM 17O WostbrockPinned" #dirname(getwd())
path.data.red <- "C:/Users/annefetrow/Library/CloudStorage/GoogleDrive-fetrowa@gmail.com/My Drive/UMich/IPL/Reactor Reductions/UM 17O WostbrockPinned" #dirname(getwd())
